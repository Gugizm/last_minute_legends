[2025-02-18T14:28:14.230+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-18T14:28:14.245+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_streaming_process_pipeline.stream_processor manual__2025-02-18T14:28:13.783219+00:00 [queued]>
[2025-02-18T14:28:14.251+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_streaming_process_pipeline.stream_processor manual__2025-02-18T14:28:13.783219+00:00 [queued]>
[2025-02-18T14:28:14.252+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2025-02-18T14:28:14.266+0000] {taskinstance.py:2889} INFO - Executing <Task(BashOperator): stream_processor> on 2025-02-18 14:28:13.783219+00:00
[2025-02-18T14:28:14.271+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'kafka_streaming_process_pipeline', 'stream_processor', 'manual__2025-02-18T14:28:13.783219+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/kafka_streaming_process.py', '--cfg-path', '/tmp/tmpsz304s_c']
[2025-02-18T14:28:14.273+0000] {standard_task_runner.py:105} INFO - Job 109: Subtask stream_processor
[2025-02-18T14:28:14.274+0000] {logging_mixin.py:190} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=1275) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2025-02-18T14:28:14.274+0000] {standard_task_runner.py:72} INFO - Started process 1277 to run task
[2025-02-18T14:28:14.307+0000] {task_command.py:467} INFO - Running <TaskInstance: kafka_streaming_process_pipeline.stream_processor manual__2025-02-18T14:28:13.783219+00:00 [running]> on host 944871224fc0
[2025-02-18T14:28:14.377+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='kafka_streaming_process_pipeline' AIRFLOW_CTX_TASK_ID='stream_processor' AIRFLOW_CTX_EXECUTION_DATE='2025-02-18T14:28:13.783219+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-18T14:28:13.783219+00:00'
[2025-02-18T14:28:14.378+0000] {taskinstance.py:731} INFO - ::endgroup::
[2025-02-18T14:28:14.391+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-18T14:28:14.393+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'python3 /usr/local/app/services/spark_streaming_processor.py']
[2025-02-18T14:28:14.400+0000] {subprocess.py:99} INFO - Output:
[2025-02-18T14:28:15.987+0000] {subprocess.py:106} INFO - /home/***/.local/lib/python3.12/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2025-02-18T14:28:16.938+0000] {subprocess.py:106} INFO - %6|1739888896.938|GETSUBSCRIPTIONS|ccloud-python-client-00816959-7fc0-4d75-98d8-88c7998cc97d#producer-1| [thrd:main]: Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to P+4684jdT2mnpSQmqQ935g
[2025-02-18T14:28:18.196+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-02-18T14:28:18.364+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 WARN DependencyUtils: Local jar /usr/local/app/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar does not exist, skipping.
[2025-02-18T14:28:18.529+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkContext: Running Spark version 3.5.4
[2025-02-18T14:28:18.529+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-02-18T14:28:18.530+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkContext: Java version 17.0.14
[2025-02-18T14:28:18.554+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceUtils: ==============================================================
[2025-02-18T14:28:18.554+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-02-18T14:28:18.555+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceUtils: ==============================================================
[2025-02-18T14:28:18.556+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkContext: Submitted application: UserActivityStreaming
[2025-02-18T14:28:18.576+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-02-18T14:28:18.584+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceProfile: Limiting resource is cpu
[2025-02-18T14:28:18.584+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-02-18T14:28:18.633+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SecurityManager: Changing view acls to: default
[2025-02-18T14:28:18.633+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SecurityManager: Changing modify acls to: default
[2025-02-18T14:28:18.634+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SecurityManager: Changing view acls groups to:
[2025-02-18T14:28:18.634+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SecurityManager: Changing modify acls groups to:
[2025-02-18T14:28:18.635+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2025-02-18T14:28:18.819+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO Utils: Successfully started service 'sparkDriver' on port 34691.
[2025-02-18T14:28:18.841+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkEnv: Registering MapOutputTracker
[2025-02-18T14:28:18.870+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkEnv: Registering BlockManagerMaster
[2025-02-18T14:28:18.885+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-02-18T14:28:18.886+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-02-18T14:28:18.889+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-02-18T14:28:18.904+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1ae0df53-20ea-4244-a21b-d71dcc285f02
[2025-02-18T14:28:18.916+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-02-18T14:28:18.930+0000] {subprocess.py:106} INFO - 25/02/18 14:28:18 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-02-18T14:28:19.056+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-02-18T14:28:19.116+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-02-18T14:28:19.126+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2025-02-18T14:28:19.156+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 ERROR SparkContext: Failed to add /usr/local/app/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar to Spark environment
[2025-02-18T14:28:19.156+0000] {subprocess.py:106} INFO - java.io.FileNotFoundException: Jar /usr/local/app/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar not found
[2025-02-18T14:28:19.157+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)
[2025-02-18T14:28:19.157+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)
[2025-02-18T14:28:19.158+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)
[2025-02-18T14:28:19.159+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)
[2025-02-18T14:28:19.159+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-02-18T14:28:19.159+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-02-18T14:28:19.160+0000] {subprocess.py:106} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-02-18T14:28:19.160+0000] {subprocess.py:106} INFO - 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:521)
[2025-02-18T14:28:19.161+0000] {subprocess.py:106} INFO - 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
[2025-02-18T14:28:19.161+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-02-18T14:28:19.161+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
[2025-02-18T14:28:19.161+0000] {subprocess.py:106} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[2025-02-18T14:28:19.162+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
[2025-02-18T14:28:19.162+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
[2025-02-18T14:28:19.162+0000] {subprocess.py:106} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
[2025-02-18T14:28:19.163+0000] {subprocess.py:106} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-02-18T14:28:19.163+0000] {subprocess.py:106} INFO - 	at py4j.Gateway.invoke(Gateway.java:238)
[2025-02-18T14:28:19.163+0000] {subprocess.py:106} INFO - 	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
[2025-02-18T14:28:19.164+0000] {subprocess.py:106} INFO - 	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
[2025-02-18T14:28:19.164+0000] {subprocess.py:106} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-02-18T14:28:19.164+0000] {subprocess.py:106} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-02-18T14:28:19.165+0000] {subprocess.py:106} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2025-02-18T14:28:19.165+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO SparkContext: Added JAR /usr/local/app/jars/snowflake-jdbc-3.22.0.jar at spark://944871224fc0:34691/jars/snowflake-jdbc-3.22.0.jar with timestamp 1739888898522
[2025-02-18T14:28:19.165+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO SparkContext: Added JAR /usr/local/app/jars/spark-snowflake_2.12-3.1.1.jar at spark://944871224fc0:34691/jars/spark-snowflake_2.12-3.1.1.jar with timestamp 1739888898522
[2025-02-18T14:28:19.209+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Starting executor ID driver on host 944871224fc0
[2025-02-18T14:28:19.210+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-02-18T14:28:19.211+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Java version 17.0.14
[2025-02-18T14:28:19.217+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/usr/local/app/jars/*,file:/tmp/***tmplqzk7_dm/*'
[2025-02-18T14:28:19.218+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@ccb4b4c for default.
[2025-02-18T14:28:19.230+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Fetching spark://944871224fc0:34691/jars/snowflake-jdbc-3.22.0.jar with timestamp 1739888898522
[2025-02-18T14:28:19.274+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO TransportClientFactory: Successfully created connection to 944871224fc0/172.18.0.5:34691 after 25 ms (0 ms spent in bootstraps)
[2025-02-18T14:28:19.281+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Utils: Fetching spark://944871224fc0:34691/jars/snowflake-jdbc-3.22.0.jar to /tmp/spark-0f9b05c5-2b21-4c3a-9693-f248dcec74cb/userFiles-76183e9f-bd27-4458-bd9d-f364891fca34/fetchFileTemp8056153825441061774.tmp
[2025-02-18T14:28:19.906+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Adding file:/tmp/spark-0f9b05c5-2b21-4c3a-9693-f248dcec74cb/userFiles-76183e9f-bd27-4458-bd9d-f364891fca34/snowflake-jdbc-3.22.0.jar to class loader default
[2025-02-18T14:28:19.907+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Fetching spark://944871224fc0:34691/jars/spark-snowflake_2.12-3.1.1.jar with timestamp 1739888898522
[2025-02-18T14:28:19.908+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Utils: Fetching spark://944871224fc0:34691/jars/spark-snowflake_2.12-3.1.1.jar to /tmp/spark-0f9b05c5-2b21-4c3a-9693-f248dcec74cb/userFiles-76183e9f-bd27-4458-bd9d-f364891fca34/fetchFileTemp606504006320888235.tmp
[2025-02-18T14:28:19.918+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Executor: Adding file:/tmp/spark-0f9b05c5-2b21-4c3a-9693-f248dcec74cb/userFiles-76183e9f-bd27-4458-bd9d-f364891fca34/spark-snowflake_2.12-3.1.1.jar to class loader default
[2025-02-18T14:28:19.926+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38687.
[2025-02-18T14:28:19.927+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO NettyBlockTransferService: Server created on 944871224fc0:38687
[2025-02-18T14:28:19.928+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-02-18T14:28:19.933+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 944871224fc0, 38687, None)
[2025-02-18T14:28:19.935+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO BlockManagerMasterEndpoint: Registering block manager 944871224fc0:38687 with 434.4 MiB RAM, BlockManagerId(driver, 944871224fc0, 38687, None)
[2025-02-18T14:28:19.938+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 944871224fc0, 38687, None)
[2025-02-18T14:28:19.939+0000] {subprocess.py:106} INFO - 25/02/18 14:28:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 944871224fc0, 38687, None)
[2025-02-18T14:28:20.270+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,270 - INFO - Snowflake Connector for Python Version: 3.12.4, Python Version: 3.12.8, Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.36
[2025-02-18T14:28:20.271+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,270 - INFO - Connecting to GLOBAL Snowflake domain
[2025-02-18T14:28:20.271+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,270 - INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-02-18T14:28:20.271+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,270 - INFO - THIS CONNECTION IS IN INSECURE MODE. IT MEANS THE CERTIFICATE WILL BE VALIDATED BUT THE CERTIFICATE REVOCATION STATUS WILL NOT BE CHECKED.
[2025-02-18T14:28:20.557+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,556 - INFO - THIS CONNECTION IS IN INSECURE MODE. IT MEANS THE CERTIFICATE WILL BE VALIDATED BUT THE CERTIFICATE REVOCATION STATUS WILL NOT BE CHECKED.
[2025-02-18T14:28:20.895+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,895 - INFO - ✅ Connected to Snowflake
[2025-02-18T14:28:20.896+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:20,895 - INFO - Starting Spark Streaming Processor...
[2025-02-18T14:28:20.901+0000] {subprocess.py:106} INFO - 25/02/18 14:28:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-02-18T14:28:20.903+0000] {subprocess.py:106} INFO - 25/02/18 14:28:20 INFO SharedState: Warehouse path is 'file:/tmp/***tmplqzk7_dm/spark-warehouse'.
[2025-02-18T14:28:22.544+0000] {subprocess.py:106} INFO - 2025-02-18 14:28:22,543 - INFO - ✅ Listening for new events...
[2025-02-18T14:28:22.555+0000] {subprocess.py:106} INFO - 25/02/18 14:28:22 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-02-18T14:30:20.353+0000] {local_task_job_runner.py:346} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2025-02-18T14:30:20.354+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-18T14:30:20.356+0000] {process_utils.py:132} INFO - Sending 15 to group 1277. PIDs of all processes in the group: [1281, 1322, 1277]
[2025-02-18T14:30:20.357+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 1277
[2025-02-18T14:30:20.358+0000] {taskinstance.py:3093} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-02-18T14:30:20.359+0000] {subprocess.py:117} INFO - Sending SIGTERM signal to process group
[2025-02-18T14:30:20.381+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-02-18T14:30:20.410+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1281, status='terminated', started='14:28:13') (1281) terminated with exit code None
[2025-02-18T14:30:20.785+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1322, status='terminated', started='14:28:14') (1322) terminated with exit code None
[2025-02-18T14:30:20.786+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=1277, status='terminated', exitcode=0, started='14:28:13') (1277) terminated with exit code 0
